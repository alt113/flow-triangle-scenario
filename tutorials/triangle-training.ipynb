{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'MiniCityScenario', 'TriangleMergeScenario']\n"
     ]
    }
   ],
   "source": [
    "import flow.scenarios as scenarios\n",
    "\n",
    "print(scenarios.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ring road scenario class\n",
    "scenario_name = \"TriangleMergeScenario\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter classes to the scenario class\n",
    "from flow.core.params import NetParams, InitialConfig\n",
    "from flow.core.params import InFlows\n",
    "\n",
    "inflow = InFlows()\n",
    "\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"inflow_highway_2\",\n",
    "           vehs_per_hour=2000,\n",
    "           departSpeed=10,\n",
    "           departLane=\"random\")\n",
    "\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"inflow_merge_2\",\n",
    "           vehs_per_hour=500,\n",
    "           departSpeed=10,\n",
    "           departLane=\"random\")\n",
    "\n",
    "additional_net_params = {\n",
    "    # length of the merge edge\n",
    "    \"merge_length\": 100,\n",
    "    # length of the highway leading to the merge\n",
    "    \"pre_merge_length\": 200,\n",
    "    # length of the highway past the merge\n",
    "    \"post_merge_length\": 100,\n",
    "    # number of lanes in the merge\n",
    "    \"merge_lanes\": 2,\n",
    "    # number of lanes in the highway\n",
    "    \"highway_lanes\": 5,\n",
    "    # max speed limit of the network\n",
    "    \"speed_limit\": 30,\n",
    "}\n",
    "\n",
    "# we choose to make the main highway slightly longer\n",
    "additional_net_params[\"pre_merge_length\"] = 150\n",
    "\n",
    "net_params = NetParams(inflows=inflow,  # our inflows\n",
    "                       no_internal_links=False,\n",
    "                       additional_params=additional_net_params)\n",
    "\n",
    "# name of the scenario\n",
    "name = \"training_triangle\"\n",
    "\n",
    "# initial configuration to vehicles\n",
    "initial_config = InitialConfig(spacing=\"random\", perturbation=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import ContinuousRouter, IDMController\n",
    "from flow.core.params import SumoCarFollowingParams, SumoLaneChangeParams\n",
    "from flow.core.params import VehicleParams\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "\n",
    "# add some vehicles to this object of type \"human\"\n",
    "vehicles.add(\n",
    "    veh_id = \"human\",\n",
    "    acceleration_controller=(IDMController, {}),\n",
    "    routing_controller = (ContinuousRouter, {}),\n",
    "    car_following_params=SumoCarFollowingParams(\n",
    "        speed_mode=\"obey_safe_speed\",\n",
    "    ),\n",
    "    lane_change_params=SumoLaneChangeParams(\n",
    "        lane_change_mode= \"strategic\",\n",
    "    ), num_vehicles = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.add(veh_id=\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams(sim_step=0.1, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "# Define horizon as a variable to ensure consistent use across notebook\n",
    "HORIZON=100\n",
    "\n",
    "env_params = EnvParams(\n",
    "    # length of one rollout\n",
    "    horizon=HORIZON,\n",
    "\n",
    "    additional_params={\n",
    "    # maximum acceleration for autonomous vehicles, in m/s^2\n",
    "    'max_accel': 3,\n",
    "    # maximum deceleration for autonomous vehicles, in m/s^2\n",
    "    'max_decel': 3,\n",
    "    # desired velocity for all vehicles in the network, in m/s\n",
    "    'target_velocity': 10,\n",
    "    # specifies whether vehicles are to be sorted by position during a\n",
    "    # simulation step. If set to True, the environment parameter\n",
    "    # self.sorted_ids will return a list of all vehicles sorted in accordance\n",
    "    # with the environment\n",
    "    'sort_vehicles': False\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Env', 'AccelEnv', 'LaneChangeAccelEnv', 'LaneChangeAccelPOEnv', 'GreenWaveTestEnv', 'GreenWaveTestEnv', 'WaveAttenuationMergePOEnv', 'BottleneckEnv', 'BottleNeckAccelEnv', 'WaveAttenuationEnv', 'WaveAttenuationPOEnv', 'TrafficLightGridEnv', 'PO_TrafficLightGridEnv', 'DesiredVelocityEnv', 'TestEnv', 'BayBridgeEnv']\n"
     ]
    }
   ],
   "source": [
    "import flow.envs as flowenvs\n",
    "\n",
    "print(flowenvs.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"AccelEnv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=name,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the scenario class the experiment uses\n",
    "    scenario=scenario_name,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=sumo_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-08_12-09-33_9465/logs.\n",
      "Waiting for redis server at 127.0.0.1:19188 to respond...\n",
      "Waiting for redis server at 127.0.0.1:60149 to respond...\n",
      "Starting the Plasma object store with 13.455396043999999 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=8a09fa0997581a823343aeb1c3e80b818f8c76d300909983\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '169.229.222.242',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-07-08_12-09-33_9465/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-07-08_12-09-33_9465/sockets/raylet'],\n",
       " 'redis_address': '169.229.222.242:19188',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=8a09fa0997581a823343aeb1c3e80b818f8c76d300909983'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [16, 16]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/2 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "\n",
      "Created LogSyncer for /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/flow/ray_results/training_triangle\n",
      "RUNNING trials:\n",
      " - PPO_AccelEnv-v0_0:\tRUNNING\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9501, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9511, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 368, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/offline/input_reader.py\", line 25, in next\n",
      "    batches = [self.sampler.get_data()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 64, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 267, in _env_runner\n",
      "    preprocessors, obs_filters, unroll_length, pack, callbacks)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 347, in _process_observations\n",
      "    filtered_obs = _get_or_raise(obs_filters, policy_id)(prep_obs)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 225, in __call__\n",
      "    self.rs.push(x)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 86, in push\n",
      "    x.shape, self._M.shape, x))\n",
      "ValueError: Unexpected input shape (6,), expected (2,), value = [0.00666858 0.33333333 0.33333333 0.13372918 0.00155556 0.00303704]\n",
      "\n",
      "\n",
      "Worker ip unknown, skipping log sync for /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm\n",
      "Attempting to recover trial state from last checkpoint.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/flow/ray_results/training_triangle\n",
      "RUNNING trials:\n",
      " - PPO_AccelEnv-v0_0:\tRUNNING, 1 failures: /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm/error_2019-07-08_12-09-47.txt\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9500, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9634, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 368, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/offline/input_reader.py\", line 25, in next\n",
      "    batches = [self.sampler.get_data()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 64, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 267, in _env_runner\n",
      "    preprocessors, obs_filters, unroll_length, pack, callbacks)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 347, in _process_observations\n",
      "    filtered_obs = _get_or_raise(obs_filters, policy_id)(prep_obs)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 225, in __call__\n",
      "    self.rs.push(x)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 86, in push\n",
      "    x.shape, self._M.shape, x))\n",
      "ValueError: Unexpected input shape (6,), expected (2,), value = [0.00381238 0.33333333 0.33333333 0.13461006 0.00155556 0.00303704]\n",
      "\n",
      "\n",
      "Worker ip unknown, skipping log sync for /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm\n",
      "Attempting to recover trial state from last checkpoint.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/flow/ray_results/training_triangle\n",
      "RUNNING trials:\n",
      " - PPO_AccelEnv-v0_0:\tRUNNING, 2 failures: /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm/error_2019-07-08_12-10-01.txt\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9637, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9728, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 368, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/offline/input_reader.py\", line 25, in next\n",
      "    batches = [self.sampler.get_data()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 64, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 267, in _env_runner\n",
      "    preprocessors, obs_filters, unroll_length, pack, callbacks)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 347, in _process_observations\n",
      "    filtered_obs = _get_or_raise(obs_filters, policy_id)(prep_obs)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 225, in __call__\n",
      "    self.rs.push(x)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 86, in push\n",
      "    x.shape, self._M.shape, x))\n",
      "ValueError: Unexpected input shape (6,), expected (2,), value = [0.01020329 0.33333333 0.33333333 0.0392031  0.00155556 0.00303704]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Worker ip unknown, skipping log sync for /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm\n",
      "Attempting to recover trial state from last checkpoint.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/flow/ray_results/training_triangle\n",
      "RUNNING trials:\n",
      " - PPO_AccelEnv-v0_0:\tRUNNING, 3 failures: /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm/error_2019-07-08_12-10-10.txt\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9731, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=9822, host=flow-All-Series)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 368, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/offline/input_reader.py\", line 25, in next\n",
      "    batches = [self.sampler.get_data()]\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 64, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 267, in _env_runner\n",
      "    preprocessors, obs_filters, unroll_length, pack, callbacks)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 347, in _process_observations\n",
      "    filtered_obs = _get_or_raise(obs_filters, policy_id)(prep_obs)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 225, in __call__\n",
      "    self.rs.push(x)\n",
      "  File \"/home/flow/anaconda3/envs/flow-master/lib/python3.5/site-packages/ray/rllib/utils/filter.py\", line 86, in push\n",
      "    x.shape, self._M.shape, x))\n",
      "ValueError: Unexpected input shape (6,), expected (2,), value = [0.00285261 0.33333333 0.33333333 0.09596698 0.00155556 0.00303704]\n",
      "\n",
      "\n",
      "Worker ip unknown, skipping log sync for /home/flow/ray_results/training_triangle/PPO_AccelEnv-v0_0_2019-07-08_12-09-33cd7cjqtm\n",
      "Attempting to recover trial state from last checkpoint.\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow-master)",
   "language": "python",
   "name": "flow-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
